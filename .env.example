# Example environment file for agent service
# Copy this to .env and fill in your values

# ========================================
# atPlatform Configuration (REQUIRED)
# ========================================

# Your agent's @sign (get from https://atsign.com)
AT_SIGN=@your_agent

# Path to your atKeys file
AT_KEYS_FILE_PATH=./keys/@your_agent_key.atKeys

# atPlatform root server (usually don't need to change)
AT_ROOT_SERVER=root.atsign.org

# ========================================
# Ollama Configuration (REQUIRED)
# ========================================

# Ollama server URL
# Use http://localhost:11434 for local installation
# Use http://ollama:11434 if running in Docker Compose
OLLAMA_HOST=http://localhost:11434

# Model to use (see https://ollama.ai/library for options)
# Popular options: llama2, mistral, codellama, phi
OLLAMA_MODEL=llama2

# ========================================
# Claude API Configuration (OPTIONAL)
# ========================================

# Get your API key from https://console.anthropic.com
# Leave empty to use Ollama only (100% private)
CLAUDE_API_KEY=

# Claude model to use
CLAUDE_MODEL=claude-3-5-sonnet-20241022

# ========================================
# Agent Behavior Configuration
# ========================================

# Agent name (displayed in chat responses)
# Leave empty to not display an agent name
# Useful when running multiple agents to identify which one responded
AGENT_NAME=

# Privacy threshold (0.0 - 1.0)
# Higher values = more processing stays local
# 0.7 means 70% confidence needed before using external LLM
PRIVACY_THRESHOLD=0.7

# Maximum context size (in tokens)
MAX_CONTEXT_SIZE=4096
